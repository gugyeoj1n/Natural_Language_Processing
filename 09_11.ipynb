{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled23.ipynb",
      "provenance": [],
      "mount_file_id": "1sccZdxUvZipctv9u_7yblGX6-xhaZNz8",
      "authorship_tag": "ABX9TyPIgpjEBZeRzlkQzGUg8wF8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gugyeoj1n/Natural_Language_Processing/blob/main/09_11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ts-gfhxdG6N5",
        "outputId": "6eaadd04-c0cf-427d-e6d8-0f6d2acaf8c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전체 문서 개수 :  2382\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "import re\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "import nltk\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/09.%20Word%20Embedding/dataset/data.csv\", filename=\"data.csv\")\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/NLP_colab/data.csv\")\n",
        "\n",
        "print(\"전체 문서 개수 : \", len(df))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _removeNonAscii(s):\n",
        "    return \"\".join(i for i in s if  ord(i)<128)\n",
        "\n",
        "def make_lower_case(text):\n",
        "    return text.lower()\n",
        "\n",
        "def remove_stop_words(text):\n",
        "    text = text.split()\n",
        "    stops = set(stopwords.words(\"english\"))\n",
        "    text = [w for w in text if not w in stops]\n",
        "    text = \" \".join(text)\n",
        "    return text\n",
        "\n",
        "def remove_html(text):\n",
        "    html_pattern = re.compile('<.*?>')\n",
        "    return html_pattern.sub(r'', text)\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    tokenizer = RegexpTokenizer(r'[a-zA-Z]+')\n",
        "    text = tokenizer.tokenize(text)\n",
        "    text = \" \".join(text)\n",
        "    return text\n",
        "\n",
        "df['cleaned'] = df['Desc'].apply(_removeNonAscii)\n",
        "df['cleaned'] = df.cleaned.apply(make_lower_case)\n",
        "df['cleaned'] = df.cleaned.apply(remove_stop_words)\n",
        "df['cleaned'] = df.cleaned.apply(remove_punctuation)\n",
        "df['cleaned'] = df.cleaned.apply(remove_html)\n",
        "df['cleaned'].replace('', np.nan, inplace=True)\n",
        "df = df[df['cleaned'].notna()]\n",
        "\n",
        "corpus = []\n",
        "for words in df['cleaned'] :\n",
        "  corpus.append(words.split())\n",
        "\n",
        "urllib.request.urlretrieve(\"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\", \\\n",
        "                           filename=\"GoogleNews-vectors-negative300.bin.gz\")\n",
        "\n",
        "word2vec_model = Word2Vec(size = 300, window=5, min_count = 2, workers = -1)\n",
        "word2vec_model.build_vocab(corpus)\n",
        "word2vec_model.intersect_word2vec_format('GoogleNews-vectors-negative300.bin.gz', lockf=1.0, binary=True)\n",
        "word2vec_model.train(corpus, total_examples = word2vec_model.corpus_count, epochs = 15)"
      ],
      "metadata": {
        "id": "kdbYqX75H1gP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_document_vectors(document_list):\n",
        "    document_embedding_list = []\n",
        "\n",
        "    for line in document_list:\n",
        "        doc2vec = None\n",
        "        count = 0\n",
        "        for word in line.split():\n",
        "            if word in word2vec_model.wv.vocab:\n",
        "                count += 1\n",
        "                if doc2vec is None:\n",
        "                    doc2vec = word2vec_model[word]\n",
        "                else:\n",
        "                    doc2vec = doc2vec + word2vec_model[word]\n",
        "\n",
        "        if doc2vec is not None:\n",
        "            doc2vec = doc2vec / count\n",
        "            document_embedding_list.append(doc2vec)\n",
        "\n",
        "    return document_embedding_list"
      ],
      "metadata": {
        "id": "Vo04NxJpIals"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_similarities = cosine_similarity(document_embedding_list, document_embedding_list)\n",
        "\n",
        "def recommendations(title):\n",
        "    books = df[['title', 'image_link']]\n",
        "\n",
        "    indices = pd.Series(df.index, index = df['title']).drop_duplicates()    \n",
        "    idx = indices[title]\n",
        "\n",
        "    sim_scores = list(enumerate(cosine_similarities[idx]))\n",
        "    sim_scores = sorted(sim_scores, key = lambda x: x[1], reverse = True)\n",
        "    sim_scores = sim_scores[1:6]\n",
        "\n",
        "    book_indices = [i[0] for i in sim_scores]\n",
        "\n",
        "    recommend = books.iloc[book_indices].reset_index(drop=True)\n",
        "\n",
        "    fig = plt.figure(figsize=(20, 30))\n",
        "\n",
        "    for index, row in recommend.iterrows():\n",
        "        response = requests.get(row['image_link'])\n",
        "        img = Image.open(BytesIO(response.content))\n",
        "        fig.add_subplot(1, 5, index + 1)\n",
        "        plt.imshow(img)\n",
        "        plt.title(row['title'])"
      ],
      "metadata": {
        "id": "Gdb7xMjjIiGK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}