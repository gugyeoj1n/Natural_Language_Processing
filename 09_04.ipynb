{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled21.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOTnF5ZtS49zAg9cU3ZuMBg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gugyeoj1n/Natural_Language_Processing/blob/main/09_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fm-2EjcaHyoR",
        "outputId": "7fd28cce-46a2-4c63-98b7-7524ef5d13c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "샘플 개수 :  11314\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  del sys.path[0]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "샘플 개수 :  10995\n",
            "단어 집합의 크기 :  64282\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
        "documents = dataset.data\n",
        "print(\"샘플 개수 : \", len(documents))\n",
        "\n",
        "news_df = pd.DataFrame({'document':documents})\n",
        "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n",
        "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: \" \".join([w for w in x.split() if len(w) > 3]))\n",
        "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())\n",
        "\n",
        "news_df.replace(\"\", float(\"NaN\"), inplace=True)\n",
        "print(news_df.isnull().values.any())\n",
        "news_df.dropna(inplace=True)\n",
        "\n",
        "print(\"샘플 개수 : \", len(news_df))\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split())\n",
        "tokenized_doc = tokenized_doc.apply(lambda x: [i for i in x if i not in stop_words])\n",
        "tokenized_doc = tokenized_doc.to_list()\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(tokenized_doc)\n",
        "\n",
        "word2idx = tokenizer.word_index\n",
        "idx2word = {value : key for key, value in word2idx.items()}\n",
        "encoded = tokenizer.texts_to_sequences(tokenized_doc)\n",
        "vocab_size = len(word2idx) + 1\n",
        "print(\"단어 집합의 크기 : \", vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
        "\n",
        "skip_grams = [skipgrams(sample, vocabulary_size=vocab_size, window_size=10) for sample in encoded]"
      ],
      "metadata": {
        "id": "_CqUMPT8Jv_l"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Embedding, Reshape, Activation, Input\n",
        "from tensorflow.keras.layers import Dot\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from IPython.display import SVG\n",
        "\n",
        "embedding_dim = 100\n",
        "w_inputs = Input(shape=(1, ), dtype='int32')\n",
        "word_embedding = Embedding(vocab_size, embedding_dim)(w_inputs)\n",
        "\n",
        "c_inputs = Input(shape=(1, ), dtype='int32')\n",
        "context_embedding = Embedding(vocab_size, embedding_dim)(c_inputs)\n",
        "\n",
        "dot_product = Dot(axes=2)([word_embedding, context_embedding])\n",
        "dot_product = Reshape((1,), input_shape=(1, 1))(dot_product)\n",
        "output = Activation('sigmoid')(dot_product)\n",
        "\n",
        "model = Model(inputs=[w_inputs, c_inputs], outputs=output)\n",
        "model.summary()\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "plot_model(model, to_file='model3.png', show_shapes=True, show_layer_names=True, rankdir='TB')\n",
        "\n",
        "for epoch in range(1, 6):\n",
        "    loss = 0\n",
        "    for _, elem in enumerate(skip_grams):\n",
        "        first_elem = np.array(list(zip(*elem[0]))[0], dtype='int32')\n",
        "        second_elem = np.array(list(zip(*elem[0]))[1], dtype='int32')\n",
        "        labels = np.array(elem[1], dtype='int32')\n",
        "        X = [first_elem, second_elem]\n",
        "        Y = labels\n",
        "        loss += model.train_on_batch(X,Y)  \n",
        "    print('Epoch :',epoch, 'Loss :',loss)"
      ],
      "metadata": {
        "id": "DtMOMXQHKN03"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}